<h1>&emsp;&emsp;&emsp;Austin Atchley | Houston Shearin<br>&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;Drew Hall | Kris Flint</h1>

  This page details what our team did for our final project in CSC 3220 Fundamentals of Data Science at Tennessee Tech University. The dataset we used can be obtained from the following link: [adult-census-income](https://www.kaggle.com/uciml/adult-census-income)


The main goal of this project was to use two different classification techniques to find which one had the highest accuracy in predicting which income bracket the individual would fall into based on his or her other features. In order to do this we had to do some data cleaning. The first step in our data cleaning was to get rid of any people that had left values empty. Then in order to allow our classifications to use all of the columns in our data set we began converting them into numeric data types, essentially assigning categorical numbers to each response for each column. The associated number values for each column as well as the download for our cleaned data set can be found [here](https://austinatchley1.github.io/Data-Science-Team-Project/Data-Cleaning.html).

The first method of classification that we used was K-NN. When we initially created our K-NN function we partitioned the training, validation, and test sets randomly through the data set. We then ran a function that randomly selected ks that seemed to be the most optimal based on the training and testing on the validation set. We then recieved and accuracy based on testing on the test set. We ran this function ten times picking the optimal k each time and testing against each test set. After this we would compare the accuracies and identify the optimal k. The average highest accuracy was around 70% and the optimal k value tended to be 21. The code and outputs that we have for knn can be found [here](https://austinatchley1.github.io/Data-Science-Team-Project/Visualization/K-NN.html).

After we created the K-NN function we found an online discussion that exposed some class bias in the training. The problem is that there is significantly more people in the lower than fifty thousand income bracket. So sometimes when we separated into the sets the training set would end up with only the first bracket. This caused shifts in the accuracy of our function. To fix this issue we had to separate the data into two different data frames, one with the first income bracket and one with the second bracket. Then we created a training set with an even amount from both brackets. Then we partitioned the rest into the validation and test sets. Now that there were equal amounts from each set we could combine the two data frames for each set together and run our functions like before. We have included both the original results and the updated results on the pages of the K-NN function.

The second method of classification that we used was logistic regression. For this classification we were aware of the bias in the data so we were able to account for it from the beginning. We split the data into training and test sets. Then when we ran the glm function we kept getting the error that y had to be between 0 and 1. In order to satisfy this condition we divided the income column by 2 which changed the values to a .5 for <=50K and a 1 for >50K. After this we could successfully run the glm function and analyze the coefficienct weights. Then we ran our model to make predictions on the test set and used this to produce a misclassification rate. The code and outputs that we have for logistic regression can be found [here](https://austinatchley1.github.io/Data-Science-Team-Project/Visualization/Code/LogisticRegression.html). 

